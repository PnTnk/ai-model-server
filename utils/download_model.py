# download_model.py
import os
import logging
import time
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoProcessor, 
    AutoModelForImageTextToText,
    AutoModelForSeq2SeqLM,
    AutoModelForVisualQuestionAnswering,
    Qwen2_5OmniForConditionalGeneration,
    Qwen2_5OmniProcessor
)
from huggingface_hub import snapshot_download, hf_hub_download
import torch

# --- ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# --- ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î ---
MODELS_TO_DOWNLOAD = {
    "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B": {
        "sub_dir": "deepseek-8b",
        "model_loader": AutoModelForCausalLM,
        "processor_loader": AutoTokenizer,
        "priority": 1,
        "description": "DeepSeek reasoning model (8B params)"
    },
    "Qwen/Qwen2.5-VL-3B-Instruct": {
        "sub_dir": "qwen-vl-3b",
        "model_loader": AutoModelForImageTextToText,
        "processor_loader": AutoProcessor,
        "priority": 2,
        "description": "Qwen vision-language model (3B params)"
    },
    "Qwen/Qwen2-Audio-7B-Instruct": {
        "sub_dir": "qwen-audio-7b",
        "model_loader": AutoModelForSeq2SeqLM,
        "processor_loader": AutoProcessor,
        "priority": 3,
        "description": "Qwen audio processing model (7B params)"
    },
    "Qwen/Qwen2.5-Omni-3B": {
        "sub_dir": "qwen-omni-3b",
        "model_loader": Qwen2_5OmniForConditionalGeneration,
        "processor_loader": Qwen2_5OmniProcessor,
        "download_method": "snapshot",
        "priority": 4,
        "description": "Qwen multimodal omni model (3B params)",
        "special_files": ["spk_dict.pt", "tokenizer.json"]  # ‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    },
    "Salesforce/blip-vqa-base": {
        "sub_dir": "blip-vqa-base",
        "model_loader": AutoModelForVisualQuestionAnswering,
        "processor_loader": AutoProcessor,
        "priority": 5,
        "description": "BLIP visual question answering model"
    },
}

# ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
BASE_SAVE_DIRECTORY = "./models"

def check_system_resources():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡πà‡∏≠‡∏ô‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î"""
    try:
        import psutil
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö RAM
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏î‡∏¥‡∏™‡∏Å‡πå
        disk = psutil.disk_usage('.')
        free_gb = disk.free / (1024**3)
        
        logging.info(f"System resources:")
        logging.info(f"  Available RAM: {available_gb:.1f} GB")
        logging.info(f"  Free disk space: {free_gb:.1f} GB")
        
        if available_gb < 8:
            logging.warning("Low RAM detected. Consider closing other applications.")
        
        if free_gb < 50:
            logging.warning("Low disk space detected. Models require ~40-50GB total.")
            
    except ImportError:
        logging.info("Install 'psutil' for system resource monitoring: pip install psutil")

def download_special_files(model_name, save_path, special_files):
    """‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
    for file_name in special_files:
        file_path = os.path.join(save_path, file_name)
        if not os.path.exists(file_path):
            try:
                logging.info(f"Downloading special file: {file_name}")
                hf_hub_download(
                    repo_id=model_name,
                    filename=file_name,
                    local_dir=save_path,
                    local_dir_use_symlinks=False
                )
                logging.info(f"Successfully downloaded: {file_name}")
            except Exception as e:
                logging.warning(f"Could not download {file_name}: {e}")
                logging.info(f"Model may work without {file_name} (limited functionality)")

def estimate_download_size(model_name):
    """‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î"""
    size_estimates = {
        "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B": "16 GB",
        "Qwen/Qwen2.5-VL-3B-Instruct": "6 GB", 
        "Qwen/Qwen2-Audio-7B-Instruct": "14 GB",
        "Qwen/Qwen2.5-Omni-3B": "12 GB",
        "Salesforce/blip-vqa-base": "2 GB",
        "lmms-lab/LLaVA-Video-7B-Qwen2": "14 GB"
    }
    return size_estimates.get(model_name, "Unknown")

def download_specific_model(model_name, model_info):
    """
    ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Memory Leak)
    """
    import gc  # ‡πÄ‡∏û‡∏¥‡πà‡∏° garbage collection
    
    save_path = os.path.join(BASE_SAVE_DIRECTORY, model_info["sub_dir"])
    
    logging.info(f"=" * 60)
    logging.info(f"Processing: {model_info.get('description', model_name)}")
    logging.info(f"Model: {model_name}")
    logging.info(f"Estimated size: {estimate_download_size(model_name)}")
    logging.info(f"Target directory: {save_path}")
    logging.info(f"=" * 60)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    config_file_path = os.path.join(save_path, 'config.json')
    if os.path.exists(config_file_path):
        logging.info("‚úì Model already exists locally. Skipping download.")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen-Omni
        if model_info.get("special_files"):
            download_special_files(model_name, save_path, model_info["special_files"])
        
        return True

    logging.info("Model not found locally. Starting download...")
    start_time = time.time()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡πá‡∏ö references
    model = None
    processor = None
    
    try:
        os.makedirs(save_path, exist_ok=True)

        # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏î‡πâ‡∏ß‡∏¢ snapshot_download ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
        if model_info.get("download_method") == "snapshot":
            logging.info(f"Using snapshot download for '{model_name}'")
            snapshot_download(
                repo_id=model_name, 
                local_dir=save_path, 
                local_dir_use_symlinks=False,
                ignore_patterns=["*.md", "*.txt", "*.gitignore"]
            )
            
            # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©
            if model_info.get("special_files"):
                download_special_files(model_name, save_path, model_info["special_files"])
        
        else:
            # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ
            model_loader_class = model_info["model_loader"]
            processor_loader_class = model_info.get("processor_loader", AutoProcessor)

            logging.info(f"Downloading processor/tokenizer...")
            processor = processor_loader_class.from_pretrained(
                model_name, 
                trust_remote_code=True,
                resume_download=True
            )
            processor.save_pretrained(save_path)
            logging.info("‚úì Processor/tokenizer downloaded")
            
            # ‡∏•‡πâ‡∏≤‡∏á processor ‡∏à‡∏≤‡∏Å memory
            del processor
            processor = None
            gc.collect()  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö garbage collection

            logging.info(f"Downloading model weights... (This may take a while)")
            model = model_loader_class.from_pretrained(
                model_name, 
                torch_dtype="auto", 
                trust_remote_code=True,
                resume_download=True,
                device_map=None,  # ‡πÑ‡∏°‡πà‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏õ‡∏ó‡∏µ‡πà GPU
                low_cpu_mem_usage=True  # ‡πÉ‡∏ä‡πâ memory ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á
            )
            model.save_pretrained(save_path)
            logging.info("‚úì Model weights downloaded")
            
            # ‡∏•‡πâ‡∏≤‡∏á model ‡∏à‡∏≤‡∏Å memory
            del model
            model = None
            gc.collect()  # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö garbage collection
        
        # ‡∏•‡πâ‡∏≤‡∏á GPU cache ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logging.info("‚úì GPU cache cleared")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
        elapsed_time = time.time() - start_time
        logging.info(f"‚úì Successfully downloaded '{model_name}' in {elapsed_time:.1f} seconds")
        
        # ‡∏•‡πâ‡∏≤‡∏á memory ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°
        gc.collect()
        
        return True

    except Exception as e:
        logging.error(f"‚úó Error downloading '{model_name}': {e}")
        logging.error("Troubleshooting tips:")
        logging.error("  1. Check internet connection")
        logging.error("  2. Verify disk space (need ~50GB total)")
        logging.error("  3. Update dependencies: pip install --upgrade transformers huggingface_hub")
        logging.error("  4. Try setting HF_HUB_ENABLE_HF_TRANSFER=1 for faster downloads")
        return False
    
    finally:
        # ‡∏•‡πâ‡∏≤‡∏á memory ‡πÉ‡∏ô finally block ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à
        if model is not None:
            del model
        if processor is not None:
            del processor
        gc.collect()
        
        # ‡∏•‡πâ‡∏≤‡∏á GPU cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡πâ‡∏≤‡∏á memory ‡∏´‡∏•‡∏±‡∏á download ‡πÄ‡∏™‡∏£‡πá‡∏à
def cleanup_memory():
    """‡∏•‡πâ‡∏≤‡∏á memory ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å download ‡πÄ‡∏™‡∏£‡πá‡∏à"""
    import gc
    
    logging.info("üßπ Cleaning up memory...")
    
    # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö garbage collection ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö
    for i in range(3):
        collected = gc.collect()
        logging.info(f"  Garbage collection round {i+1}: freed {collected} objects")
    
    # ‡∏•‡πâ‡∏≤‡∏á GPU cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        logging.info("  ‚úì GPU cache cleared")
    
    # ‡πÅ‡∏™‡∏î‡∏á memory usage ‡∏´‡∏•‡∏±‡∏á‡∏•‡πâ‡∏≤‡∏á
    try:
        import psutil
        process = psutil.Process()
        memory_mb = process.memory_info().rss / (1024 * 1024)
        logging.info(f"  Current memory usage: {memory_mb:.1f} MB")
    except ImportError:
        pass
    
    logging.info("‚úì Memory cleanup completed")

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô main()
def main():
    """
    ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    """
    logging.info("üöÄ Starting model download process...")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏£‡∏∞‡∏ö‡∏ö
    check_system_resources()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å
    if not os.path.exists(BASE_SAVE_DIRECTORY):
        os.makedirs(BASE_SAVE_DIRECTORY)
        logging.info(f"Created base directory: {BASE_SAVE_DIRECTORY}")

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏° priority
    sorted_models = sorted(
        MODELS_TO_DOWNLOAD.items(),
        key=lambda x: x[1].get("priority", 999)
    )
    
    successful_downloads = 0
    total_models = len(sorted_models)
    
    for model_name, model_info in sorted_models:
        if download_specific_model(model_name, model_info):
            successful_downloads += 1
        
        # ‡∏•‡πâ‡∏≤‡∏á memory ‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
        cleanup_memory()
        
        # ‡∏û‡∏±‡∏Å‡∏´‡∏≤‡∏¢‡πÉ‡∏à‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î
        time.sleep(2)
        
    logging.info("=" * 60)
    logging.info(f"üìä Download Summary:")
    logging.info(f"  Successful: {successful_downloads}/{total_models}")
    logging.info(f"  Failed: {total_models - successful_downloads}/{total_models}")
    
    if successful_downloads == total_models:
        logging.info("üéâ All models downloaded successfully!")
    else:
        logging.warning("‚ö†Ô∏è  Some models failed to download. Check logs above.")
    
    # ‡∏•‡πâ‡∏≤‡∏á memory ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    cleanup_memory()
    
    logging.info("üèÅ Model download process finished.")

if __name__ == "__main__":
    main()